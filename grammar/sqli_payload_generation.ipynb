{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iIqakVF1ebsr"
   },
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<unk>\": 1,\n",
    "    \"<bos>\": 2,\n",
    "    \"<eos>\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9WS-I-sVX9Rn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "idx = 0 # starting vocab id\n",
    "\n",
    "def load_grammar_from_file(filename):\n",
    "    grammar = {}\n",
    "    current_rule = None\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('//'):  # Skip comments and empty lines\n",
    "                continue\n",
    "            line = line[:-1]\n",
    "            # Check for a rule definition\n",
    "            rule_match = re.match(r'([a-zA-Z0-9_]+)\\s*:\\s*(.*)', line)\n",
    "            if rule_match:\n",
    "                current_rule = rule_match.group(1)\n",
    "                expansions = rule_match.group(2).split('|')  # Split different productions\n",
    "                grammar[current_rule] = [expansion.strip() for expansion in expansions]\n",
    "            # Continuation of rules on the next line (sometimes multiline rules)\n",
    "            elif current_rule:\n",
    "                expansions = line.split('|')\n",
    "                grammar[current_rule].extend([expansion.strip() for expansion in expansions])\n",
    "\n",
    "    return grammar\n",
    "\n",
    "def remove_first_last_quote(input_string):\n",
    "    if input_string.startswith(\"'\") and input_string.endswith(\"'\"):\n",
    "        return input_string[1:-1]\n",
    "    return input_string\n",
    "\n",
    "def replace_escaped_quote(input_string):\n",
    "    if input_string == \"\\\\'\":\n",
    "        return \"'\"\n",
    "    return input_string\n",
    "\n",
    "def generate(rule):\n",
    "    if rule in grammar:\n",
    "        expansion = random.choice(grammar[rule]).split()\n",
    "\n",
    "        original_payload = []\n",
    "        tokenized_payload = []\n",
    "\n",
    "        for token in expansion:\n",
    "            original, token_ids = generate(token)\n",
    "            original_payload.append(original)\n",
    "            tokenized_payload.extend(token_ids)\n",
    "\n",
    "        return ''.join(original_payload), tokenized_payload\n",
    "    else:\n",
    "        processed_rule = replace_escaped_quote(remove_first_last_quote(rule))\n",
    "        if processed_rule not in vocab:\n",
    "            vocab[processed_rule] = len(vocab)\n",
    "\n",
    "        return processed_rule, [vocab[processed_rule]]\n",
    "\n",
    "grammar = load_grammar_from_file('grammar/SQLi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krKytvjYD5G_"
   },
   "outputs": [],
   "source": [
    "def token_ids_to_original_payload(tokenized_payload, vocab):\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    original_payload = [id_to_token[token_id] for token_id in tokenized_payload]\n",
    "\n",
    "    return ''.join(original_payload)\n",
    "\n",
    "for i in range(100):\n",
    "    sqli = generate('start')\n",
    "    if sqli[0]!=token_ids_to_original_payload(sqli[1], vocab):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLsGk6NLnJ65",
    "outputId": "534e1c57-87ac-4057-bf7a-9be93a4e2c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '0': 4, '%27': 5, '%0b': 6, 'and': 7, '+': 8, '!': 9, '~': 10, 'false': 11, 'or': 12, \"'\": 13, '': 14, 'is': 15, '6': 16, '1': 17, '<@=1.': 18, '&&': 19, 'not': 20, '%7e': 21, '!@<@': 22, 'true': 23, '=': 24, '(': 25, ')': 26, '%23': 27, '<@!=1.': 28, ';': 29, 'select': 30, 'sleep': 31, '--': 32, 'extractvalue': 33, ',': 34, 'concat': 35, '0x7e': 36, '@@version': 37, '%2C': 38, '9': 39, '@<@': 40, '2': 41, '1<@': 42, '{a': 43, '1}=1': 44, 'union': 45, '5': 46, '{x': 47, '1)}=1': 48, '<@=.1': 49, 'updatexml': 50, '4': 51, '8': 52, '<@!=.1': 53, '@<@.': 54, '-': 55, '<@=1': 56, '{`if`': 57, 'like': 58, '3': 59, '<@!=1': 60, '<': 61, '!@<@.': 62, '7': 63, '1<@.': 64}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gun4Su-EYRBc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output = []\n",
    "tokenized_output = []\n",
    "num_payloads = 1000000\n",
    "sqli_set = set()\n",
    "\n",
    "\n",
    "for i in range(num_payloads):\n",
    "    while True:\n",
    "        sqli = generate('start')\n",
    "        if sqli[0] not in sqli_set:\n",
    "            sqli_set.add(sqli[0])\n",
    "            tokenized_output.append(sqli[1])\n",
    "            output.append(f\"{sqli[0]}\\n\")\n",
    "            if sqli[0] != token_ids_to_original_payload(sqli[1], vocab):\n",
    "                print(i)\n",
    "            break\n",
    "\n",
    "# Writing to file in one go\n",
    "with open('sqli.txt', 'w') as f:\n",
    "    f.writelines(output)\n",
    "with open('tokenized_sqli.json', 'w') as f:\n",
    "    json.dump(tokenized_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "imjQAGdNpTGZ"
   },
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as json_file:\n",
    "    json.dump(vocab, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5CYZkTPbEen",
    "outputId": "5718eb50-a1d2-4efe-d340-65516190ed44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "print(len(sqli_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<unk>\": 1,\n",
    "    \"<bos>\": 2,\n",
    "    \"<eos>\": 3\n",
    "}\n",
    "\n",
    "import random\n",
    "import re\n",
    "idx = 0 # starting vocab id\n",
    "\n",
    "def load_grammar_from_file(filename):\n",
    "    grammar = {}\n",
    "    current_rule = None\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('//'):  # Skip comments and empty lines\n",
    "                continue\n",
    "            line = line[:-1]\n",
    "            # Check for a rule definition\n",
    "            rule_match = re.match(r'([a-zA-Z0-9_]+)\\s*:\\s*(.*)', line)\n",
    "            if rule_match:\n",
    "                current_rule = rule_match.group(1)\n",
    "                expansions = rule_match.group(2).split('|')  # Split different productions\n",
    "                grammar[current_rule] = [expansion.strip() for expansion in expansions]\n",
    "            # Continuation of rules on the next line (sometimes multiline rules)\n",
    "            elif current_rule:\n",
    "                expansions = line.split('|')\n",
    "                grammar[current_rule].extend([expansion.strip() for expansion in expansions])\n",
    "\n",
    "    return grammar\n",
    "\n",
    "def remove_first_last_quote(input_string):\n",
    "    if input_string.startswith(\"'\") and input_string.endswith(\"'\"):\n",
    "        return input_string[1:-1]\n",
    "    return input_string\n",
    "\n",
    "def replace_escaped_quote(input_string):\n",
    "    if input_string == \"\\\\'\":\n",
    "        return \"'\"\n",
    "    return input_string\n",
    "\n",
    "def generate(rule):\n",
    "    if rule in grammar:\n",
    "        expansion = random.choice(grammar[rule]).split()\n",
    "\n",
    "        original_payload = []\n",
    "        tokenized_payload = []\n",
    "\n",
    "        for token in expansion:\n",
    "            original, token_ids = generate(token)\n",
    "            original_payload.append(original)\n",
    "            tokenized_payload.extend(token_ids)\n",
    "\n",
    "        return ''.join(original_payload), tokenized_payload\n",
    "    else:\n",
    "        processed_rule = replace_escaped_quote(remove_first_last_quote(rule))\n",
    "        if processed_rule not in vocab:\n",
    "            vocab[processed_rule] = len(vocab)\n",
    "\n",
    "        return processed_rule, [vocab[processed_rule]]\n",
    "\n",
    "grammar = load_grammar_from_file('grammar/SQLi.txt')\n",
    "\n",
    "def token_ids_to_original_payload(tokenized_payload, vocab):\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    original_payload = [id_to_token[token_id] for token_id in tokenized_payload]\n",
    "\n",
    "    return ''.join(original_payload)\n",
    "\n",
    "for i in range(100):\n",
    "    sqli = generate('start')\n",
    "    if sqli[0]!=token_ids_to_original_payload(sqli[1], vocab):\n",
    "        print(i)\n",
    "\n",
    "import json\n",
    "\n",
    "output = []\n",
    "tokenized_output = []\n",
    "num_payloads = 1000000\n",
    "sqli_set = set()\n",
    "\n",
    "\n",
    "for i in range(num_payloads):\n",
    "    while True:\n",
    "        sqli = generate('start')\n",
    "        if sqli[0] not in sqli_set:\n",
    "            sqli_set.add(sqli[0])\n",
    "            tokenized_output.append(sqli[1])\n",
    "            output.append(f\"{sqli[0]}\\n\")\n",
    "            if sqli[0] != token_ids_to_original_payload(sqli[1], vocab):\n",
    "                print(i)\n",
    "            break\n",
    "\n",
    "# Writing to file in one go\n",
    "with open('sqli.txt', 'w') as f:\n",
    "    f.writelines(output)\n",
    "with open('tokenized_sqli.json', 'w') as f:\n",
    "    json.dump(tokenized_output, f)\n",
    "\n",
    "with open('vocab.json', 'w') as json_file:\n",
    "    json.dump(vocab, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
